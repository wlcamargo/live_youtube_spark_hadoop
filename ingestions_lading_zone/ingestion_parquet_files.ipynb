{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4204ff7-1de1-4c44-a0ea-eaf165ee7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingestão Ficheiros Landing Zone \n",
    "\n",
    "### Leitura dos ficheiros\n",
    "### Ingestão no formato parquet na camada landing zone do HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00304c6-eb62-4d91-93d4-52180324f4b1",
   "metadata": {},
   "source": [
    "## Importação das LIBS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65dd9577-25ca-4672-a3f4-63f23fc1a569",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "from configs import config_env_test\n",
    "from functions import functions as func\n",
    "\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"LocalDelta\") \\\n",
    "    .master(\"local\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711501e0-28bb-4d92-8f8d-81e3c014b2d8",
   "metadata": {},
   "source": [
    "## Conexao com Postgres\n",
    "\n",
    "##  *** Ideal é utilizar o jar do Postgres para conectar ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4b2227-29a2-41a6-9e3c-a6fa6bf7a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie a conexão usando psycopg2\n",
    "conn_postgres = psycopg2.connect(**config_env_test.credential_postgres_adventureworks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ddadf-dbda-4dba-bce6-c4ea29baa91b",
   "metadata": {},
   "source": [
    "## Ingestão no Data Lake - Camada Landing Zone - Formato Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19ebd9e-4c77-4e1d-a6c2-ef060beb1bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 18:05:11,073 - INFO - Starting ingestions from adventureworks to HDFS...\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:11,166 - INFO - Processing table: sales_countryregioncurrency\n",
      "2024-01-16 18:05:14,787 - INFO - Table sales_countryregioncurrency successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_countryregioncurrency\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:14,825 - INFO - Processing table: sales_creditcard\n",
      "2024-01-16 18:05:15,892 - INFO - Table sales_creditcard successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_creditcard\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:15,900 - INFO - Processing table: sales_currency\n",
      "2024-01-16 18:05:16,112 - INFO - Table sales_currency successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_currency\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:16,160 - INFO - Processing table: sales_currencyrate\n",
      "2024-01-16 18:05:17,070 - INFO - Table sales_currencyrate successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_currencyrate\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:17,130 - INFO - Processing table: sales_customer\n",
      "2024-01-16 18:05:18,595 - INFO - Table sales_customer successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_customer\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:18,622 - INFO - Processing table: sales_personcreditcard\n",
      "2024-01-16 18:05:19,256 - INFO - Table sales_personcreditcard successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_personcreditcard\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:19,784 - INFO - Processing table: sales_salesorderdetail\n",
      "2024-01-16 18:05:26,539 - INFO - Table sales_salesorderdetail successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salesorderdetail\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:26,547 - INFO - Processing table: sales_salestaxrate\n",
      "2024-01-16 18:05:26,758 - INFO - Table sales_salestaxrate successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salestaxrate\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:26,795 - INFO - Processing table: sales_salesorderheadersalesreason\n",
      "2024-01-16 18:05:27,590 - INFO - Table sales_salesorderheadersalesreason successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salesorderheadersalesreason\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:27,592 - INFO - Processing table: sales_salesperson\n",
      "2024-01-16 18:05:27,785 - INFO - Table sales_salesperson successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salesperson\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:27,788 - INFO - Processing table: sales_salespersonquotahistory\n",
      "2024-01-16 18:05:27,988 - INFO - Table sales_salespersonquotahistory successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salespersonquotahistory\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:27,991 - INFO - Processing table: sales_salesreason\n",
      "2024-01-16 18:05:28,170 - INFO - Table sales_salesreason successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salesreason\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:28,173 - INFO - Processing table: sales_salestaxrate\n",
      "2024-01-16 18:05:28,348 - INFO - Table sales_salestaxrate successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salestaxrate\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:28,351 - INFO - Processing table: sales_salesterritory\n",
      "2024-01-16 18:05:28,558 - INFO - Table sales_salesterritory successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salesterritory\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:28,562 - INFO - Processing table: sales_salesterritoryhistory\n",
      "2024-01-16 18:05:28,761 - INFO - Table sales_salesterritoryhistory successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_salesterritoryhistory\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:28,764 - INFO - Processing table: sales_shoppingcartitem\n",
      "2024-01-16 18:05:29,397 - INFO - Table sales_shoppingcartitem successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_shoppingcartitem\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:29,399 - INFO - Processing table: sales_specialoffer\n",
      "2024-01-16 18:05:29,983 - INFO - Table sales_specialoffer successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_specialoffer\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:29,987 - INFO - Processing table: sales_specialofferproduct\n",
      "2024-01-16 18:05:30,178 - INFO - Table sales_specialofferproduct successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_specialofferproduct\n",
      "/tmp/ipykernel_18660/2874189151.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn_postgres)\n",
      "2024-01-16 18:05:30,182 - INFO - Processing table: sales_store\n",
      "2024-01-16 18:05:30,389 - INFO - Table sales_store successfully processed and saved to HDFS: hdfs://namenode:9000/landing_zone/sales_store\n",
      "2024-01-16 18:05:30,390 - INFO - ingestions from adventureworks to HDFS completed!\n"
     ]
    }
   ],
   "source": [
    "# Configure the logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Log the start of ingestions\n",
    "logging.info(\"Starting ingestions from adventureworks to HDFS...\")\n",
    "\n",
    "for key, value in config_env_test.tables_postgres_adventureworks.items():\n",
    "    table_postgres = value\n",
    "    table_name_hdfs = func.convert_table_name(table_postgres)\n",
    "    \n",
    "    try:\n",
    "        # Build the SQL query\n",
    "        query = f\"SELECT * FROM {table_postgres}\"\n",
    "        \n",
    "        # Use pandas to read the query results directly into a DataFrame\n",
    "        df = pd.read_sql_query(query, conn_postgres)\n",
    "\n",
    "        hdfs_path = config_env_test.hdfs_path['landing_zone']\n",
    "        \n",
    "        target = f\"{hdfs_path}{table_name_hdfs}\"\n",
    "        logging.info(f\"Processing table: {table_name_hdfs}\")\n",
    "\n",
    "        df_spark = spark.createDataFrame(df)\n",
    "            \n",
    "        df_spark.write.format(\"parquet\").mode(\"overwrite\").save(target)\n",
    "        logging.info(f\"Table {table_name_hdfs} successfully processed and saved to HDFS: {target}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any exceptions\n",
    "        logging.error(f\"Error processing table {table_name_hdfs}: {str(e)}\")\n",
    "\n",
    "# Optionally, you can add logging messages for the end of the script\n",
    "logging.info(\"ingestions from adventureworks to HDFS completed!\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
